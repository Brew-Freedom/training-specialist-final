AI Training Data Interactive Demo

Overview

This project is an interactive demonstration of AI training data concepts. Users can:

Test a Sentiment Analyzer in real time.

Train the AI by adding new positive or negative words.

Check the quality of CSV datasets (empty rows, duplicates).

Run a Before/After Dataset Demo to see how data cleaning improves accuracy.

Explore optional demos like Numeric Prediction and Image Labeling.

Learning takeaway: This project shows how the quality of training data directly impacts AI predictions, giving hands-on experience with training, testing, and debugging AI models.

How to Use / User Process
1. Test the Sentiment Analyzer

Type a sentence under the Sentiment Analyzer section.

Click Check Sentiment.

Observe the AIâ€™s prediction: Positive, Negative, or Neutral.

Learning takeaway: Users see how word choice in the training data affects AI predictions.

2. Train the AI by Adding Words

Enter a new word in Train the Sentiment AI.

Select Positive or Negative.

Click Add Word.

Test the Sentiment Analyzer with sentences containing the new word.

Learning takeaway: Shows that AI can learn from new training data, changing its predictions.

3. Check Dataset Quality

Upload a CSV file in Data Quality Checker.

Click Analyze Dataset.

View results:

Total rows

Empty rows

Duplicate rows

Check the interactive bar chart showing dataset statistics.

Learning takeaway: Demonstrates how dataset quality (missing or duplicate data) affects AI performance.

4. Run Before/After Dataset Demo

Click Run Demo under Before/After Dataset Demo.

See simulated AI accuracy before and after cleaning the dataset.

Learning takeaway: Shows the impact of data cleaning on AI predictions.

5. Optional Interactive Demos

Numeric Prediction:

Enter a number, click Predict, and see Low / Medium / High.

Image Labeling:

Upload an image, click Label Image, and see a simulated label (Cat, Dog, Car, etc.).

Learning takeaway: Users can explore different types of AI predictions to understand general AI behavior and the role of training data.

Quality Assurance (QA)

Manual Testing: Users test each input and output to ensure correct predictions.

Debugging: Errors in input handling are caught and displayed.

Automated Testing: Scripts can validate word addition, sentiment predictions, and dataset analysis.

Regression Testing: Ensures updates to training words or new demos do not break existing functionality.
